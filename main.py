import requests
import os
import re
import time
import json
import random
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from bs4 import BeautifulSoup
from threading import Lock, Thread
from queue import Queue
import logging
from logging.handlers import RotatingFileHandler
from tenacity import retry, stop_after_attempt, wait_random_exponential, retry_if_exception_type
from collections import defaultdict

# ==============================================================================
# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ÛŒÙ†Ú¯ ---
# ==============================================================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
handler = RotatingFileHandler('app.log', maxBytes=1024*1024, backupCount=5)
handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
logger.addHandler(handler)

# ==============================================================================
# --- Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙˆÙˆÚ©Ø§Ù…Ø±Ø³ Ùˆ Ø³Ø§ÛŒØª Ù…Ø¨Ø¯Ø§ ---
# ==============================================================================
BASE_URL = "https://panel.eways.co"
PRODUCT_DETAIL_URL_TEMPLATE = f"{BASE_URL}/Store/Detail/{{cat_id}}/{{product_id}}"

EWAYS_USERNAME = os.environ.get("EWAYS_USERNAME") or "Ø´Ù…Ø§Ø±Ù‡ Ù…ÙˆØ¨Ø§ÛŒÙ„ ÛŒØ§ ÛŒÙˆØ²Ø±Ù†ÛŒÙ…"
EWAYS_PASSWORD = os.environ.get("EWAYS_PASSWORD") or "Ù¾Ø³ÙˆØ±Ø¯"
CACHE_FILE = 'products_cache.json'  # ÙØ§ÛŒÙ„ Ú©Ø´

# ==============================================================================
# --- ØªØ§Ø¨Ø¹ Ù„Ø§Ú¯ÛŒÙ† Ø§ØªÙˆÙ…Ø§ØªÛŒÚ© Ø¨Ù‡ eways ---
# ==============================================================================
def login_eways(username, password):
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': f"{BASE_URL}/",
        'X-Requested-With': 'XMLHttpRequest',
        'Accept-Language': 'en-US,en;q=0.9,fa;q=0.8'
    })
    session.verify = False

    login_url = f"{BASE_URL}/User/Login"
    payload = {
        "UserName": username,
        "Password": password,
        "RememberMe": "true"
    }
    logger.info("â³ Ø¯Ø± Ø­Ø§Ù„ Ù„Ø§Ú¯ÛŒÙ† Ø¨Ù‡ Ù¾Ù†Ù„ eways ...")
    resp = session.post(login_url, data=payload, timeout=30)
    if resp.status_code != 200:
        logger.error(f"âŒ Ù„Ø§Ú¯ÛŒÙ† Ù†Ø§Ù…ÙˆÙÙ‚! Ú©Ø¯ ÙˆØ¶Ø¹ÛŒØª: {resp.status_code} - Ù…ØªÙ† Ù¾Ø§Ø³Ø®: {resp.text[:200]}")
        return None

    if 'Aut' in session.cookies:
        logger.info("âœ… Ù„Ø§Ú¯ÛŒÙ† Ù…ÙˆÙÙ‚! Ú©ÙˆÚ©ÛŒ Aut Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯.")
        return session
    else:
        logger.error("âŒ Ú©ÙˆÚ©ÛŒ Aut Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø§Ú¯ÛŒÙ† Ù†Ø§Ù…ÙˆÙÙ‚ ÛŒØ§ Ø¯Ù„ÛŒÙ„ Ù†Ø§Ù…Ø´Ø®Øµ.")
        return None

# ==============================================================================
# --- Ú¯Ø±ÙØªÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø­ØµÙˆÙ„ (specs) ---
# ==============================================================================
@retry(
    retry=retry_if_exception_type(requests.exceptions.RequestException),
    stop=stop_after_attempt(3),
    wait=wait_random_exponential(multiplier=1, max=5),
    reraise=True
)
def get_product_details(session, cat_id, product_id):
    url = PRODUCT_DETAIL_URL_TEMPLATE.format(cat_id=cat_id, product_id=product_id)
    try:
        response = session.get(url, timeout=60)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'lxml')
        specs_table = soup.select_one('#link1 .table-responsive table')
        if not specs_table:
            specs_table = soup.select_one('.table-responsive table')
            if not specs_table:
                specs_table = soup.find('table', class_='table')
                if not specs_table:
                    return {}
        specs = {}
        rows = specs_table.find_all("tr")
        for row in rows:
            cells = row.find_all("td")
            if len(cells) == 2:
                key = cells[0].text.strip()
                value = cells[1].text.strip()
                if key and value:
                    specs[key] = value
        return specs
    except Exception as e:
        logger.warning(f"      - Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø´Ø®ØµØ§Øª Ù…Ø­ØµÙˆÙ„ {product_id}: {e}")
        return {}

# ==============================================================================
# --- Ú¯Ø±ÙØªÙ† Ù…Ø­ØµÙˆÙ„Ø§Øª Ù‡Ø± Ø¯Ø³ØªÙ‡ (ÙÙ‚Ø· Ù…ÙˆØ¬ÙˆØ¯Ù‡Ø§ Ùˆ ØªÙˆÙ‚Ù Ù‡ÙˆØ´Ù…Ù†Ø¯) ---
# ==============================================================================
@retry(
    retry=retry_if_exception_type((requests.exceptions.RequestException, requests.exceptions.HTTPError)),
    stop=stop_after_attempt(4),
    wait=wait_random_exponential(multiplier=1, max=10),
    reraise=True
)
def get_products_from_category_page(session, category_id, max_pages=10, delay=0.5):
    all_products_in_category = []
    seen_product_ids = set()
    page = 1
    error_count = 0
    while page <= max_pages:
        # --- Ù…Ø­ØµÙˆÙ„Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ HTML ---
        if page == 1:
            url = f"{BASE_URL}/Store/List/{category_id}/2/2/0/0/0/10000000000?text=%DA%AF%D9%88%D8%B4%DB%8C-%D9%85%D9%88%D8%A8%D8%A7%DB%8C%D9%84"
        else:
            url = f"{BASE_URL}/Store/List/{category_id}/2/2/{page-1}/0/0/10000000000?brands=&isMobile=false"
        logger.info(f"â³ Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØµÙˆÙ„Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ Ø§Ø² HTML ØµÙØ­Ù‡ {page} ...")
        try:
            resp = session.get(url, timeout=30)
            if resp.status_code != 200:
                logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª HTML ØµÙØ­Ù‡ {page} - status: {resp.status_code} - url: {url}")
                break
            soup = BeautifulSoup(resp.text, 'lxml')
            product_blocks = soup.select(".goods-record")
            html_products = []
            for block in product_blocks:
                a_tag = block.select_one("a")
                name_tag = block.select_one("span.goods-record-title")
                unavailable = block.select_one(".goods-record-unavailable")
                is_available = unavailable is None
                if a_tag and name_tag:
                    product_id = None
                    href = a_tag['href']
                    match = re.search(r'/Store/Detail/\d+/(\d+)', href)
                    if match:
                        product_id = match.group(1)
                    name = name_tag.text.strip()
                    price_tag = block.select_one("span.goods-record-price")
                    price_text = price_tag.text.strip() if price_tag else ""
                    price = re.sub(r'[^\d]', '', price_text) if price_text else "0"
                    image_tag = block.select_one("img.goods-record-image")
                    image_url = image_tag.get('data-src', '') if image_tag else ''
                    if is_available and product_id and product_id not in seen_product_ids:
                        specs = get_product_details(session, category_id, product_id)
                        html_products.append({
                            'id': product_id,
                            'name': name,
                            'category_id': category_id,
                            'price': price,
                            'stock': 1,
                            'image': image_url,
                            'specs': specs,
                        })
                        seen_product_ids.add(product_id)
                        time.sleep(random.uniform(delay, delay + 0.2))
            logger.info(f"ğŸŸ¢ Ù…Ø­ØµÙˆÙ„Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø§ÙˆÙ„ÛŒÙ‡ (HTML) ØµÙØ­Ù‡ {page}: {len(html_products)}")

            # --- Ù…Ø­ØµÙˆÙ„Ø§Øª Lazy ---
            lazy_products = []
            lazy_page = 1
            referer_url = url
            while True:
                data = {
                    "ListViewType": 0,
                    "CatId": category_id,
                    "Order": 2,
                    "Sort": 2,
                    "LazyPageIndex": lazy_page,
                    "PageIndex": page - 1,
                    "PageSize": 24,
                    "Available": 0,
                    "MinPrice": 0,
                    "MaxPrice": 10000000000,
                    "IsLazyLoading": "true"
                }
                headers = {
                    "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
                    "X-Requested-With": "XMLHttpRequest",
                    "Referer": referer_url
                }
                logger.info(f"â³ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª LazyPageIndex={lazy_page} ØµÙØ­Ù‡ {page} ...")
                resp = session.post(f"{BASE_URL}/Store/ListLazy", data=data, headers=headers, timeout=30)
                if resp.status_code != 200:
                    logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØµÙˆÙ„Ø§Øª (Ú©Ø¯: {resp.status_code})")
                    break
                try:
                    result = resp.json()
                except Exception as e:
                    logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ Ù¾Ø§Ø³Ø® Ø¨Ù‡ json: {e}")
                    logger.error(f"Ù…ØªÙ† Ù¾Ø§Ø³Ø® Ø³Ø±ÙˆØ±:\n{resp.text[:500]}")
                    break
                if not result or "Goods" not in result or not result["Goods"]:
                    logger.info(f"ğŸš© Ø¨Ù‡ Ø§Ù†ØªÙ‡Ø§ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª Lazy ØµÙØ­Ù‡ {page} Ø±Ø³ÛŒØ¯ÛŒÙ… ÛŒØ§ Ù„ÛŒØ³Øª Ø®Ø§Ù„ÛŒ Ø§Ø³Øª.")
                    break
                goods = result["Goods"]
                for g in goods:
                    if g.get("Availability", True):
                        product_id = str(g["Id"])
                        if product_id not in seen_product_ids:
                            specs = get_product_details(session, category_id, product_id)
                            lazy_products.append({
                                "id": product_id,
                                "name": g["Name"],
                                "category_id": category_id,
                                "price": g.get("Price", "0"),
                                "stock": 1,
                                "image": g.get("ImageUrl", ""),
                                "specs": specs,
                            })
                            seen_product_ids.add(product_id)
                            time.sleep(random.uniform(delay, delay + 0.2))
                logger.info(f"ğŸŸ¢ Ù…Ø­ØµÙˆÙ„Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø§ÛŒÙ† ØµÙØ­Ù‡ Lazy: {len([g for g in goods if g.get('Availability', True)])}")
                lazy_page += 1
            # Ø¬Ù…Ø¹ Ù…Ø­ØµÙˆÙ„Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø§ÛŒÙ† ØµÙØ­Ù‡
            available_in_page = html_products + lazy_products
            if not available_in_page:
                logger.info(f"â›”ï¸ Ù‡ÛŒÚ† Ù…Ø­ØµÙˆÙ„ Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø¯Ø± ØµÙØ­Ù‡ {page} Ù†Ø¨ÙˆØ¯. Ø¨Ø±Ø±Ø³ÛŒ ØµÙØ­Ø§Øª Ù…ØªÙˆÙ‚Ù Ø´Ø¯.")
                break
            all_products_in_category.extend(available_in_page)
            page += 1
            error_count = 0
        except Exception as e:
            error_count += 1
            logger.error(f"    - Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙØ­Ù‡ Ù…Ø­ØµÙˆÙ„Ø§Øª: {e} (ØªØ¹Ø¯Ø§Ø¯ Ø®Ø·Ø§: {error_count})")
            if error_count >= 3:
                logger.critical(f"ğŸš¨ ØªØ¹Ø¯Ø§Ø¯ Ø®Ø·Ø§Ù‡Ø§ÛŒ Ù…ØªÙˆØ§Ù„ÛŒ Ø¯Ø± Ø¯Ø³ØªÙ‡ {category_id} Ø¨Ù‡ {error_count} Ø±Ø³ÛŒØ¯! ØªÙˆÙ‚Ù Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§ÛŒÙ† Ø¯Ø³ØªÙ‡.")
                break
            time.sleep(2)
    logger.info(f"    - ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù…Ø­ØµÙˆÙ„Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡ Ø§Ø² Ø¯Ø³ØªÙ‡ {category_id}: {len(all_products_in_category)}")
    return all_products_in_category

# ==============================================================================
# --- Ú©Ø´ Ø¨Ø±Ø§ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª (Ú©Ù„ÛŒØ¯ ØªØ±Ú©ÛŒØ¨ÛŒ id|category_id) ---
# ==============================================================================
def load_cache():
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, 'r') as f:
            cache = json.load(f)
            logger.info(f"âœ… Ú©Ø´ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯. ØªØ¹Ø¯Ø§Ø¯ Ù…Ø­ØµÙˆÙ„Ø§Øª Ø¯Ø± Ú©Ø´: {len(cache)}")
            return cache
    logger.info("âš ï¸ Ú©Ø´ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù…Ù„ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
    return {}

def save_cache(products):
    with open(CACHE_FILE, 'w') as f:
        json.dump(products, f, ensure_ascii=False, indent=4)
    logger.info(f"âœ… Ú©Ø´ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯. ØªØ¹Ø¯Ø§Ø¯ Ù…Ø­ØµÙˆÙ„Ø§Øª: {len(products)}")

# ==============================================================================
# --- ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ ---
# ==============================================================================
def main():
    session = login_eways(EWAYS_USERNAME, EWAYS_PASSWORD)
    if not session:
        logger.error("âŒ Ù„Ø§Ú¯ÛŒÙ† Ø¨Ù‡ Ù¾Ù†Ù„ eways Ø§Ù†Ø¬Ø§Ù… Ù†Ø´Ø¯. Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø®Ø§ØªÙ…Ù‡ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯.")
        return

    category_id = 4286
    products = get_products_from_category_page(session, category_id, max_pages=10)
    logger.info(f"\nâœ… ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù…Ø­ØµÙˆÙ„Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø§ÛŒÙ† Ø¯Ø³ØªÙ‡: {len(products)}\n")
    for i, p in enumerate(products, 1):
        print(f"{i:03d}. {p['name']} (ID: {p['id']}) | Ù‚ÛŒÙ…Øª: {p['price']} | Ø¹Ú©Ø³: {p['image']} | ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: {json.dumps(p['specs'], ensure_ascii=False)}")

    # Ú©Ø´ Ù‚Ø¨Ù„ÛŒ Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†
    cached_products = load_cache()
    updated_products = {}
    changed_count = 0

    for p in products:
        key = f"{p['id']}|{p['category_id']}"
        if key in cached_products and cached_products[key]['price'] == p['price'] and cached_products[key]['stock'] == p['stock']:
            updated_products[key] = cached_products[key]
        else:
            updated_products[key] = p
            changed_count += 1
            # Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ ÙˆÙˆÚ©Ø§Ù…Ø±Ø³ ÛŒØ§ Ù‡Ø± Ø¬Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒ

    save_cache(updated_products)

if __name__ == "__main__":
    main()
